SQL Query steps:
- QueryEngine::new()
- query_engine.run_sql()
- conductor.run_sql_from_string()
- conductor.run_sql()
- lp: from_sql()
- pp: logical_plan_to_physical_plan()
- conductor.run_physical_plan()
- executor.configure_query()
- executor.execute()
- opiterator.open()
- opiterator.next()

In logical_plan_to_physical_plan(), the function calls on db_state.catalog 
DatabaseState handles creation/loading of a database which calls upon the 
storage manager which subsequently reads a page from the HeapFile.


- For nested_loop_join, I kept track of open and current_tuple. If the iterator
was open, every time next() was called, I would have a current_tuple from the 
outer table (left child) and I would iterate through all the tuples from the 
inner table (right child). If the evaluated left and right tuples matched by the 
given operator, I merged the two. Once all the right_tuples were iterated through, 
I move onto the next tuple in the outer relation and repeat the process until 
all tuples from the outer table have been passed through.
For the hash_join, I kept track of open and current_tuple along with a hash
of all the outer table tuples. When I opened the iterator, I created a hashing
between each tuple from the outer table and its evaluated value. If multiple tuples
hashed to the same value, I added it to a vector. This time,
the current_tuple is from the inner table. Then, in next(), I iterate through
all the inner table tuples by using the evaluated expression as a hash function
for the key of the hashmap of the outer table. If there was a value found,
I merged the two tuples. For values that matched multiple tuples, I merged
all the left child tuples with the current right child tuple
For aggregate, I kept track of open, a hashmap of the accumulated values, the current
group being iterated on, and the keys of the hashmap. The hashmap had the groups as keys 
and the values were an accumulation of each aggregate operator for tuples added to
the group. I implemented the accumulation as a Vec<(Field, Field)> to account for the Avg
operator. I chose to keep track of the keys so I could iterate through the hashmap with an 
index and allow for rewinding.

- This milestone took me around 10 hours. Understanding how the open and next functions operated
took me the longest. Once I understood how tuples were being iterated through, the logic for
implementing the query operators was more straightforward. Aggregate took me the longest 
and much of it was to account for Avg. Additionally, figuring out the hidden hash join test 
took a significant amount of time. I enjoyed this project, especially thinking through
which states to keep track of.
